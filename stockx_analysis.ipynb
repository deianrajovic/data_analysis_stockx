{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oHDrNXOKIp9m"
   },
   "source": [
    "# Data Analysis and Visualization | IT Coding, Tools and Security Project"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tfIspXuUHcg8"
   },
   "source": [
    "## Introduction to Data Analysis and Visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "X_p8l4V2HTe1"
   },
   "source": [
    "The complex enviroment in which the companies are operating nowadays, require to sense when and how change happens in order to be ready repond to that change, preventing threats and leveraging opportunities.\n",
    "\n",
    "Data analysis and visualization are processes that  allow to clean, transform and visualize large set of raw data,to better understand their meaning. Through these techniques it is possible to transform large and complex datasets in visive information, making them easy to understand even by who is not familiar with data analysis. Its main purose is to simplify the methods to identify tendencies, data analisys models and data anomalies when we are dealing with big data.\n",
    "\n",
    "In order to do so, companies collect an enormous amount of data from a lot of differente sources, both internal and external, that need to be stored and processed to extract useful information. The processes for store, analyze and distribute these data require an adequate IT infrastructes with a good combination of hardware and software solutions. This architecture should be designed considering the company's business objectives and should have the following charatestics:\n",
    "\n",
    "* **Scalability**: to address the problem of always growin data.\n",
    "* **Parallel processing**: in order to let data to be accessed in multiple ways from multiple places and at multiple rates of speed.\n",
    "* **Low-latency resources**: reducing the time to store, process and deliver data at the minimum level.\n",
    "* **Data optimization**: an optimal data architecture enables data independence and provides a simple, resilient, and agile environment to support its analysis.\n",
    "\n",
    "Therefore, a good IT architecture for Data Analysis and Visualization should have the right balance between performance, availability and access to data. Performance are usually limited by the costs, the availability should be based on a good resiliency to failures and access should meet the business organization requirements.\n",
    "\n",
    "In terms of software and cloud applications, there are numerous solutions that visualize data allowing to extract information in very easy, fast and intuitable way. Some examples are Tebleau, SAS, Splunk, and QlikView, or open-source tools such as Apache Hadoop, Spark, and Hive.\n",
    "But in order to design applications that better fit business intelligence requirements, the use of an object-oriented programming language like Python is very common to streamline large complex dataset. Python offers multiple data analysis and visualization libraries that are rich of different features, allowing to create highly customizable and interactive representations that go beyond the standard charts. The most used ones are: Numpy, Pandas, Plotly, SeaBorn, ggPlot and Matplotlib.\n",
    "\n",
    "The combination of Python libraries allows Data Analyst to develop instant reports, interactive and real-time dashboards to better present their studies to company’s stakeholders.\n",
    "\n",
    "The typical data analysis process is made by: Collection, Cleaning, Exploratory Analysis and Visualization, and the building and deployment of the model for more complex project (Data Sceince, Machine Learning, ...)\n",
    "\n",
    "Through this project we want to give a simplified example of how powerful and useful the processes of data analysis and visualization through Python are when we analyze a set of structured data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "594aXWUoHgrS"
   },
   "source": [
    "## Project overview"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Sk_TMxBCGmID"
   },
   "source": [
    "The dataset that we used come from StockX, a startup company that built a resell marketplace for buying and selling sneakers, watches and clothing. The company is born 5 years ago and is expanding at an incredible speed, now counting more than 800 employees. Since the primary market of Stockx are sneakers, our dataset is based on two of the most famous brands on that market of recent years, Yeezy and “Nike x Off-White”.  The structured data provided by the company includes information on:\n",
    "\n",
    "* **Order Date**\n",
    "* **Sneaker Brand and Model**\n",
    "* **Price of retail**\n",
    "* **Price of sale**\n",
    "* **Release date**\n",
    "* **Country**\n",
    "\n",
    "All these information are related to the time span that start the first September 2017 and end the 2019, for a total of x months.\n",
    "\n",
    "\n",
    "The project is diveded in three sections: Data Cleaning, Exploratory Data Analysis, Data Visualization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZM6a37SbIzA5"
   },
   "source": [
    "## Step 1: Data cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "b-FhkqhIJoha"
   },
   "source": [
    "  First of all we made some data cleaning in order to have a dataset that is uniform on its wholeness and that can be better managed in order to facilitate the following work on it. \n",
    "  Data Cleaning is a fundamental part of analysis and analytics, especially if we have to visualize data. A tidy dataset allows limit mistakes and make the charts, plots, and graphs closer to reality and with an higher aesthetic quality.\n",
    "\n",
    "  In this step, we will:\n",
    "\n",
    "  * Remove null rows with null values (unless the values can be predicted or approximated)\n",
    "  * Ensure that each columns describe a variable and each rows an elements or observation\n",
    "  * Make the values be the proper type, to make operations easier\n",
    "  * Delete misspellings or other related error that can be an obstacle for subsequent activities\n",
    "  * Sort values by date and by brand"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OfUkgj3fKS2X"
   },
   "outputs": [],
   "source": [
    "#Invoking the modules needed and reading the dataset\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import os\n",
    "import datetime\n",
    "import plotly.express as px\n",
    "import json\n",
    "from matplotlib import pyplot as plt\n",
    "from urllib.request import urlopen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "n226mwkkoTox",
    "outputId": "cbef0465-f814-4b94-9b76-fe0e0584d28b"
   },
   "outputs": [],
   "source": [
    "#navigate to the directory where the file is saved\n",
    "os.chdir(\"/Users/deianrajovic/Desktop/Further Learning/Data Science/Dataset\")\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 204
    },
    "id": "5sI_a-0zKbYe",
    "outputId": "eee9eb09-f1af-4f60-b2e3-757afba19078"
   },
   "outputs": [],
   "source": [
    "#reading csv\n",
    "df = pd.read_csv(\"stockx.csv\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4RjfAkXyMRwO"
   },
   "source": [
    "As you can see, the dataset at the current state cannot be used for the analysis. For these reason in the following cells we're going to clean it. \n",
    "Messy rows have NaN values in every columns but the \"Order Date\", in which all the observations are reported."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 204
    },
    "id": "7Hb-bvK9MnuC",
    "outputId": "d5cf05d2-d99d-4195-8885-5c376abd20f5"
   },
   "outputs": [],
   "source": [
    "#creating a temporary pdf to rearrange the biased columns\n",
    "null_df = df[df.Brand.isnull() == True]\n",
    "df = df[df.Brand.isnull() == False]\n",
    "\n",
    "null_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AVjUdrCrNCUH"
   },
   "outputs": [],
   "source": [
    "#spliting and renaming\n",
    "order_date = null_df[\"Order Date\"]\n",
    "null_df = order_date.str.split(\",\", expand=True).rename(columns={\n",
    "    0 :'Order Date', 1 : 'Brand', 2 : 'Sneaker Name', 3: 'SalePrice1', 4 : 'SalePrice2', 5 : 'Retail Price', 6 : 'Release Date', \n",
    "    7 : 'Shoe Size', 8 : 'Buyer Region'})\n",
    "\n",
    "#casting\n",
    "null_df[\"Shoe Size\"] = null_df[\"Shoe Size\"].apply(pd.to_numeric)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 204
    },
    "id": "z6OKfFJKN5Dw",
    "outputId": "afbf5598-4485-45ae-9fe3-d85278023be1"
   },
   "outputs": [],
   "source": [
    "#merging the two sale price columns\n",
    "saleprice = null_df['SalePrice1'].astype(str) + null_df['SalePrice2'].astype(str)\n",
    "null_df.insert(3, 'Sale Price', saleprice)\n",
    "null_df.drop(['SalePrice1','SalePrice2'],axis=1, inplace=True)\n",
    "null_df['Sale Price'] = null_df['Sale Price'].str.replace('\"','', regex=True)\n",
    "\n",
    "null_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1xSTMigWN61D"
   },
   "outputs": [],
   "source": [
    "#merging the initial df with the reorganized null_df\n",
    "df1 = df.append(null_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-8kBIUWcOMAp"
   },
   "outputs": [],
   "source": [
    "# date columns\n",
    "df1[\"Order Date\"] = pd.to_datetime(df1[\"Order Date\"])\n",
    "df1[\"Release Date\"] = pd.to_datetime(df1[\"Release Date\"])\n",
    "\n",
    "# clean or change the name of the brands\n",
    "df1[\"Brand\"].replace({\"Off-White\": \"Nike x OW\", \" Yeezy\": \"Yeezy\"}, inplace=True)\n",
    "\n",
    "# sneaker Name column\n",
    "df1[\"Sneaker Name\"] = df1[\"Sneaker Name\"].str.replace(\"-\", \" \")\n",
    "\n",
    "# price columns\n",
    "df1[\"Sale Price\"] = df1[\"Sale Price\"].str.replace(\"$\", \"\", regex=False)\n",
    "df1[\"Retail Price\"] = df1[\"Retail Price\"].str.replace(\"$\", \"\", regex=False)\n",
    "\n",
    "# casting the price columns to numeric in order to make mathematical and statistical operations\n",
    "df1[[\"Sale Price\", \"Retail Price\"]] = df1[[\"Sale Price\", \"Retail Price\"]].apply(pd.to_numeric)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 204
    },
    "id": "jPnZ-aKGOktY",
    "outputId": "b39fe390-16f4-430d-8350-f620ee9a7dbe"
   },
   "outputs": [],
   "source": [
    "df1.sort_values([\"Brand\", \"Order Date\"], ascending=True, inplace=True)\n",
    "df1.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "R1UanBWfWSbw"
   },
   "source": [
    "This is the cleaned dataframe in which we are going to work in the next steps."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2Mnt0vWcPgmL"
   },
   "source": [
    "## Step 2: Exploratory data analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BaHpmYmJRjWP"
   },
   "source": [
    "In this step, through the use of numpy and standard numpy functions, we are going to create some variables representing common statistic values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ts1Gc3h-oJAL"
   },
   "source": [
    "Exploratory Data Analysis (EDA) is an approach/philosophy for data analysis that employs a variety of techniques (mostly graphical) to:\n",
    "1. Maximize insight into a data set; 2. uncover underlying structure.\n",
    "3. Extract important variables.\n",
    "4. Detect outliers and anomalies.\n",
    "5. Test underlying assumptions.\n",
    "6. Develop parsimonious models.\n",
    "7. Determine optimal factor settings.\n",
    "\n",
    "The particular graphical techniques employed in EDA are often quite simple, consisting of various techniques of:\n",
    "1. Plotting the raw data: such as data traces, histograms, bihistograms, probability plots, lag plots, block plots, and Youden plots.\n",
    "2. Plotting simple statistics: such as mean plots, standard deviation plots, box plots, and main effects plots of the raw data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rUmummnQToUF"
   },
   "outputs": [],
   "source": [
    "yeezy = df1[df1.Brand == \"Yeezy\"]\n",
    "nike = df1[df1.Brand == \"Nike x OW\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bPm-rshsTw3p"
   },
   "source": [
    "It is better to create two different dataframe for each brand, in order to make some exploratory data analysis. We will go through every individual column and find out relevant summary statistics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "g9QltXUUT8sL",
    "outputId": "bf509cae-83f7-4c5c-9d06-419aac18ecdf"
   },
   "outputs": [],
   "source": [
    "# ORDER DATE\n",
    "\n",
    "# Yeezy\n",
    "first_day_yeezy = yeezy[\"Order Date\"].min().date()\n",
    "last_day_yeezy = yeezy[\"Order Date\"].max().date()\n",
    "first_day_yeezy = datetime.datetime.strptime(str(first_day_yeezy), \"%Y-%m-%d\").strftime(\"%d/%m/%Y\")\n",
    "last_day_yeezy = datetime.datetime.strptime(str(last_day_yeezy), \"%Y-%m-%d\").strftime(\"%d/%m/%Y\")\n",
    "print(\"The first day available in the yeezy sub-dataset is \" + str(first_day_yeezy) + \". The last day, is \" + str(last_day_yeezy) + \".\")\n",
    "\n",
    "# Nike\n",
    "first_day_nike = nike[\"Order Date\"].min().date()\n",
    "last_day_nike = nike[\"Order Date\"].max().date()\n",
    "first_day_nike = datetime.datetime.strptime(str(first_day_nike), \"%Y-%m-%d\").strftime(\"%d/%m/%Y\")\n",
    "last_day_nike = datetime.datetime.strptime(str(last_day_nike), \"%Y-%m-%d\").strftime(\"%d/%m/%Y\")\n",
    "print(\"The first day available in the nike sub-dataset is \" + str(first_day_nike) + \". The last day, is \" + str(last_day_nike) + \".\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "G44a7rWBULt6",
    "outputId": "0fa8837f-b6eb-4ea3-dfc3-afc1934b1589"
   },
   "outputs": [],
   "source": [
    "# BRAND\n",
    "\n",
    "# Count\n",
    "yeezy_percentage = len(yeezy) / len(df1)\n",
    "nike_percentage = len(nike) / len(df1)\n",
    "\n",
    "print()\n",
    "print(\"In the period under consideration, the total sales of Yeezy have been \" + str(f'{len(yeezy):,}') + \", that is the \" + f'{yeezy_percentage:.2%}' + \" of the total number of sales. \")\n",
    "\n",
    "print(\"Instead, Nike x Off-White sneakers have reached a total of \" + str(f'{len(nike):,}') + \" sales, \" + \"counting only for the \" + f\"{nike_percentage:.2%}\"+\".\")\n",
    "print()\n",
    "\n",
    "# Percentage\n",
    "\n",
    "# counting yeezy model sales and find percentage over the total\n",
    "yeezy_models = yeezy[\"Sneaker Name\"].value_counts().to_frame().reset_index()\n",
    "yeezy_models.rename(columns={\"index\": \"Sneaker Model\",'Sneaker Name':'Count'}, inplace=True)\n",
    "yeezy_models[\"Percentage\"] = yeezy_models[\"Count\"] / len(yeezy)\n",
    "yeezy_models[\"Percentage\"] = pd.Series([f'{val:.2%}' for val in yeezy_models[\"Percentage\"]])\n",
    "\n",
    "# counting nike model sales and find percentage over the total\n",
    "nike_models = nike[\"Sneaker Name\"].value_counts().to_frame().reset_index()\n",
    "nike_models.rename(columns={\"index\": \"Sneaker Model\", 'Sneaker Name':'Count'}, inplace=True)\n",
    "nike_models[\"Percentage\"] = nike_models[\"Count\"] / len(nike)\n",
    "nike_models[\"Percentage\"] = pd.Series([f'{val:.2%}' for val in nike_models[\"Percentage\"]])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Ap5dNtzaUaio",
    "outputId": "26d6cbad-fdf2-4839-aedf-5b85420b1e7a"
   },
   "outputs": [],
   "source": [
    "# SNEAKER NAME\n",
    "\n",
    "# selecting iconic model and calculating number of sales and percentage on total brand sales\n",
    "\n",
    "# yeezy 350 (first gen.)\n",
    "yeezy_350 = yeezy.loc[yeezy['Sneaker Name'].str.contains('V2') == False]\n",
    "\n",
    "\n",
    "yeezy_350_count = len(yeezy_350)\n",
    "yeezy_350_percentage = yeezy_350_count / len(yeezy)\n",
    "\n",
    "\n",
    "# yeezy 350 V2\n",
    "yeezy_350v2 = yeezy.loc[yeezy['Sneaker Name'].str.contains('V2')]\n",
    "\n",
    "\n",
    "yeezy_350v2_count = len(yeezy_350v2)\n",
    "yeezy_350v2_percentage = yeezy_350v2_count / len(yeezy)\n",
    "\n",
    "print(\"YEEZY:\\n\\n\",\n",
    "      \"Yeezy 350 V2 have been sold\",yeezy_350v2_count,\"sales during the entire period considered,\",f\"{yeezy_350v2_percentage:.2%}\",\"of total brand sales.\\n\", \n",
    "      \"The iconic model of the first generation, instead, counts only for\", yeezy_350_count, \"sales.\\n\\n This enormous gap is probably given by the fact that the price is too high (since the lower stock number).\")\n",
    "\n",
    "# jordan 1\n",
    "nike_jordan1 = nike.loc[nike['Sneaker Name'].str.contains('Air Jordan 1')]\n",
    "\n",
    "nike_jordan1_count = len(nike_jordan1)\n",
    "nike_jordan1_percentage = nike_jordan1_count / len(nike)\n",
    "\n",
    "\n",
    "# air force\n",
    "nike_airforce1 = nike.loc[nike['Sneaker Name'].str.contains('Nike Air Force 1')]\n",
    "\n",
    "nike_airforce1_count = len(nike_airforce1)\n",
    "nike_airforce1_percentage = nike_airforce1_count / len(nike)\n",
    "print()\n",
    "print (\"NIKE:\\n\\n\"\n",
    "      \" The total of Jordan 1 x Off White sold is\",nike_jordan1_count,\"pairs, which is\",f\"{nike_jordan1_percentage:.2%}\",\"of the total.\",\n",
    "       \"While Nike Air Force 1 x Off White is\",f\"{nike_airforce1_percentage:.2%}\"+\".\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 145
    },
    "id": "bdhHz7OdXZPj",
    "outputId": "d626074d-44b7-4cba-d600-a255ecbf7067"
   },
   "outputs": [],
   "source": [
    "#SALE PRICE\n",
    "\n",
    "#mean\n",
    "yeezy_avg_saleprice = round(yeezy['Sale Price'].mean(), ndigits=2)\n",
    "nike_avg_saleprice = round(nike['Sale Price'].mean(), ndigits=2)\n",
    "\n",
    "#q1\n",
    "yeezy_q1_saleprice = np.quantile(yeezy[\"Sale Price\"], 0.25)\n",
    "nike_q1_saleprice = np.quantile(nike[\"Sale Price\"], 0.25)\n",
    "\n",
    "\n",
    "#median\n",
    "yeezy_med_saleprice = round(yeezy['Sale Price'].median(), ndigits=2)\n",
    "nike_med_saleprice = round(nike['Sale Price'].median(), ndigits=2)\n",
    "\n",
    "#q3\n",
    "yeezy_q3_saleprice = np.quantile(yeezy[\"Sale Price\"], 0.75)\n",
    "nike_q3_saleprice = np.quantile(nike[\"Sale Price\"], 0.75)\n",
    "\n",
    "#IQR\n",
    "yeezy_iqr_saleprice = yeezy_q3_saleprice - yeezy_q1_saleprice\n",
    "nike_iqr_saleprice = nike_q3_saleprice - nike_q1_saleprice\n",
    "\n",
    "#Max and min\n",
    "yeezy_min_saleprice = round(yeezy['Sale Price'].min(), ndigits=2)\n",
    "nike_min_saleprice = round(nike['Sale Price'].min(), ndigits=2)\n",
    "\n",
    "yeezy_max_saleprice = round(yeezy['Sale Price'].max(), ndigits=2)\n",
    "nike_max_saleprice = round(nike['Sale Price'].max(), ndigits=2)\n",
    "\n",
    "#Range\n",
    "yeezy_range_saleprice = yeezy_max_saleprice - yeezy_min_saleprice\n",
    "nike_range_saleprice = nike_max_saleprice - nike_min_saleprice\n",
    "\n",
    "#Variance\n",
    "yeezy_var_saleprice = round(yeezy['Sale Price'].var(), ndigits=2)\n",
    "nike_var_saleprice = round(nike['Sale Price'].var(), ndigits=2)\n",
    "\n",
    "#Standard deviation\n",
    "yeezy_std_saleprice = round(yeezy['Sale Price'].std(), ndigits=2)\n",
    "nike_std_saleprice = round(nike['Sale Price'].std(), ndigits=2)\n",
    "\n",
    "\n",
    "#representing them in a dataframe\n",
    "print('Basic statistics on sale price data:\\n')\n",
    "describe_saleprice = pd.DataFrame([[\"Nike\", nike_avg_saleprice, nike_q1_saleprice, nike_med_saleprice, nike_q3_saleprice, nike_iqr_saleprice, nike_min_saleprice, f'{nike_max_saleprice:,}', f'{nike_range_saleprice:,}', f'{nike_var_saleprice:,}', nike_std_saleprice], \n",
    "                                    [\"Yeezy\", yeezy_avg_saleprice, yeezy_q1_saleprice, yeezy_med_saleprice, yeezy_q3_saleprice, yeezy_iqr_saleprice, yeezy_min_saleprice, f'{yeezy_max_saleprice:,}', f'{yeezy_range_saleprice:,}', f'{yeezy_var_saleprice:,}', yeezy_std_saleprice]], \n",
    "                                    columns=[\"Brand\", \"Mean\", \"Q1\", \"Median\", \"Q3\", \"IQR\", \"Min\", \"Max\", \"Range\", \"Variance\", \"Standard Dev.\"])\n",
    "describe_saleprice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 128
    },
    "id": "Pbx-RPFPZF-e",
    "outputId": "94058235-3cd1-4ab0-cadc-9ce55f951f31"
   },
   "outputs": [],
   "source": [
    "#SHOE SIZE\n",
    "\n",
    "#mean\n",
    "yeezy_avg_size = round(yeezy['Shoe Size'].mean(), ndigits=2)\n",
    "nike_avg_size = round(nike['Shoe Size'].mean(), ndigits=2)\n",
    "\n",
    "#q1\n",
    "yeezy_q1_size = np.quantile(yeezy[\"Shoe Size\"], 0.25)\n",
    "nike_q1_size = np.quantile(nike[\"Shoe Size\"], 0.25)\n",
    "\n",
    "\n",
    "#median\n",
    "yeezy_med_size = round(yeezy['Shoe Size'].median(), ndigits=2)\n",
    "nike_med_size = round(nike['Shoe Size'].median(), ndigits=2)\n",
    "\n",
    "#q3\n",
    "yeezy_q3_size = np.quantile(yeezy[\"Shoe Size\"], 0.75)\n",
    "nike_q3_size = np.quantile(nike[\"Shoe Size\"], 0.75)\n",
    "\n",
    "#IQR\n",
    "yeezy_iqr_size = yeezy_q3_size - yeezy_q1_size\n",
    "nike_iqr_size = nike_q3_size - nike_q1_size\n",
    "\n",
    "#Max and min\n",
    "yeezy_min_size = round(yeezy['Shoe Size'].min(), ndigits=2)\n",
    "nike_min_size = round(nike['Shoe Size'].min(), ndigits=2)\n",
    "\n",
    "yeezy_max_size = round(yeezy['Shoe Size'].max(), ndigits=2)\n",
    "nike_max_size = round(nike['Shoe Size'].max(), ndigits=2)\n",
    "\n",
    "#Range\n",
    "yeezy_range_size = yeezy_max_size - yeezy_min_size\n",
    "nike_range_size = nike_max_size - nike_min_size\n",
    "\n",
    "#Variance\n",
    "yeezy_var_size = round(yeezy['Shoe Size'].var(), ndigits=2)\n",
    "nike_var_size= round(nike['Shoe Size'].var(), ndigits=2)\n",
    "\n",
    "#Standard deviation\n",
    "yeezy_std_size = round(yeezy['Shoe Size'].std(), ndigits=2)\n",
    "nike_std_size = round(nike['Shoe Size'].std(), ndigits=2)\n",
    "print()\n",
    "print('Basic statistics on shoe size distribution:\\n')\n",
    "describe_size = pd.DataFrame([[\"Nike\", nike_q1_size, nike_med_size, nike_q3_size, nike_min_size,nike_max_size], \n",
    "                        [\"Yeezy\", yeezy_q1_size, yeezy_med_size, yeezy_q3_size, yeezy_min_size,yeezy_max_size]], \n",
    "                        columns=[\"Brand\", \"Q1\", \"Median\", \"Q3\", \"Min\", \"Max\",])\n",
    "\n",
    "describe_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 221
    },
    "id": "kS-MTb0DZyed",
    "outputId": "79080635-3fcd-4e1e-daed-35f2ad500665"
   },
   "outputs": [],
   "source": [
    "# BUYER REGION\n",
    "\n",
    "yeezy_regions_count = yeezy['Buyer Region'].value_counts()\n",
    "nike_regions_count = nike['Buyer Region'].value_counts()\n",
    "total_regions_count = np.add(yeezy_regions_count, nike_regions_count)\n",
    "regions = yeezy['Buyer Region'].unique()\n",
    "\n",
    "buyer_regions = pd.DataFrame({'Yeezy' : yeezy_regions_count,'Nike x OW' : nike_regions_count, 'Total orders' : total_regions_count}, \n",
    "                             index = regions)\n",
    "print()\n",
    "print('The following dataframe is showing the best 5 countries for number of orders:\\n')\n",
    "buyer_regions.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7MV-3cikUf-E"
   },
   "source": [
    "## Step 3: Data Visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "90YjNsCmoJAP"
   },
   "source": [
    "Data analysis and visualization are processes that  allow to clean, transform and visualize large set of raw data,to better understand their meaning. Through these techniques it is possible to transform large and complex datasets in visive information, making them easy to understand even by who is not familiar with data analysis. Its main purose is to simplify the methods to identify tendencies, data analisys models and data anomalies when we are dealing with big data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gPNwj5mxUkL5"
   },
   "source": [
    "Using the data previusly cleaned and partially elaborated, we are going to process some visualization to better undestand for each brand the:\n",
    "- Best-selling models for each brand\n",
    "- Distrubution of sale price for each size of each Brand\n",
    "- Geographical distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rO_1HKbQUick"
   },
   "source": [
    "The three following visualizations show which are the best models for each brand and what is the sale price distribution according to shoe size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "59CjK4x1QdwJ"
   },
   "outputs": [],
   "source": [
    "sns.set_theme(context='notebook')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 432
    },
    "id": "1SSzDvZCWFdK",
    "outputId": "f7bf7e03-c9e4-4fe0-caba-73c2807fd626"
   },
   "outputs": [],
   "source": [
    "# donut chart best-selling nike\n",
    "\n",
    "#grouping top 5 Nike models and Nike others\n",
    "nike_models_main = nike_models.loc[:4]\n",
    "nike_models_main_count = list(nike_models_main['Count'].astype(int))\n",
    "nike_models_others = nike_models.loc[5:]\n",
    "nike_models_others_count = [nike_models_others['Count'].sum().astype(int)]\n",
    "nike_models_pie = nike_models_main_count + nike_models_others_count\n",
    "\n",
    "# chart\n",
    "plt.pie(x=nike_models_pie,\n",
    "        radius = 1.75,\n",
    "        colors = ['#ddf2d8', '#c2e7c0', '#95d6bb', '#61bdcd', '#3597c4', '#0d6dae'],\n",
    "        autopct = '%0.2f%%',\n",
    "        pctdistance=0.80,\n",
    "        startangle = 345,\n",
    "        shadow = False,\n",
    "        textprops={'color':\"#ffffff\"}\n",
    "        )\n",
    "\n",
    "centre_circle = plt.Circle((0,0),1.1,fc='white')\n",
    "fig = plt.gcf()\n",
    "fig.gca().add_artist(centre_circle)\n",
    "\n",
    "plt.axis='equal'\n",
    "plt.title('Best-Selling Nike x OW Models', fontdict = {'fontsize': 15}, y=1.4,x=1)\n",
    "plt.legend(title = 'Sneaker Model',\n",
    "          bbox_to_anchor=(1.45, 0,1, 1),\n",
    "          loc = 'center left',\n",
    "          labels = ['Jordan 1 Retro High UNC',\n",
    "                    'Air Presto Black','Air Presto White','Vapormax',\n",
    "                    'Blazer Mid Hallows Eve','Others']\n",
    "           )\n",
    "print()\n",
    "plt.show()\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 433
    },
    "id": "M-FSeTdsWbmj",
    "outputId": "d1cd9b22-0413-4c70-bebf-fe2086a3589b"
   },
   "outputs": [],
   "source": [
    "# donut chart best-selling yeezy\n",
    "\n",
    "# grouping top 5 Yeezy models and Yeezy others\n",
    "yeezy_models_main = yeezy_models.loc[:4]\n",
    "yeezy_models_main_count = list(yeezy_models_main['Count'].astype(int))\n",
    "yeezy_models_others = yeezy_models.loc[5:]\n",
    "yeezy_models_others_count = [yeezy_models_others['Count'].sum().astype(int)]\n",
    "yeezy_models_pie = yeezy_models_main_count + yeezy_models_others_count\n",
    "\n",
    "# chart\n",
    "plt.pie(x=yeezy_models_pie,\n",
    "        radius = 1.75,\n",
    "        colors = ['#ddf2d8', '#c2e7c0', '#95d6bb', '#61bdcd', '#3597c4', '#0d6dae'],\n",
    "        autopct = '%0.2f%%',\n",
    "        pctdistance=0.80,\n",
    "        startangle = 345,\n",
    "        shadow = False,\n",
    "        textprops={'color':\"#ffffff\"}\n",
    "        )\n",
    "\n",
    "centre_circle = plt.Circle((0,0),1.1,fc='white')\n",
    "fig = plt.gcf()\n",
    "fig.gca().add_artist(centre_circle)\n",
    "\n",
    "plt.axis='equal'\n",
    "plt.title('Best-Selling Yeezy Models', fontdict = {'fontsize': 15}, y=1.4,x=1)\n",
    "plt.legend(title = 'Sneaker Model',\n",
    "          bbox_to_anchor=(1.45, 0,1, 1),\n",
    "          loc = 'center left',\n",
    "          labels = ['Boost 350 V2 Butter','Boost 350 V2 Beluga',\n",
    "                   'Boost 350 V2 Zebra','Boost 350 V2 Blue Tint',\n",
    "                   'Boost 350 V2 Cream White','Others']\n",
    "           )\n",
    "print()\n",
    "plt.show()\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 637
    },
    "id": "7ED-i61EX9Q6",
    "outputId": "36c594a1-ec85-46b7-f8f5-5503d311a534"
   },
   "outputs": [],
   "source": [
    "# distribution of sales x size\n",
    "\n",
    "joint = sns.jointplot(data=df1, \n",
    "              y='Sale Price',\n",
    "              x='Shoe Size',\n",
    "              height=7.5,\n",
    "              alpha=0.9,\n",
    "              hue = 'Brand',\n",
    "              palette = \"YlGnBu\"\n",
    "              )\n",
    "\n",
    "joint.set_axis_labels(\"Shoe Size\", \"Sale Price\", fontsize=15, labelpad=20)\n",
    "joint.fig.suptitle(\"Sales-Size Distribution\", y=1.05, fontsize=20)\n",
    "\n",
    "\n",
    "\n",
    "print()\n",
    "plt.show(joint)\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "s0_U9wfFSTjG"
   },
   "source": [
    "The geographic map and heatmap models below show us that California, New York, Oregon and Florida are the states where most of the orders have been placed. \n",
    "We can also notice the enourmous gap between the numbers of those regions versus central regions like Montana, Wyoming, South/North Dakota. \n",
    "\n",
    "The reason could be related to multiple economical, social and demographic variables, for sure states like NY and CA are much more linked with the fashion industry and the effects can be seen even throught these maps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 542
    },
    "id": "_y0XroJ1ahxl",
    "outputId": "9e24dba0-3858-4435-e4bd-a6d2f5a24f16"
   },
   "outputs": [],
   "source": [
    "#Map Total Sales\n",
    "\n",
    "buyer_regions.reset_index(inplace=True)\n",
    "buyer_regions.rename(columns={0 : 'State Name'})\n",
    "buyer_regions.sort_values(by=\"Total orders\")\n",
    "buyer_regions\n",
    "\n",
    "#importing json map template\n",
    "states = json.load(urlopen('https://raw.githubusercontent.com/PublicaMundi/MappingAPI/master/data/geojson/us-states.json'))\n",
    "\n",
    "#giving id numb for each state\n",
    "state_id_map = {}\n",
    "for feature in states['features']:\n",
    "  feature['id'] = feature['id']\n",
    "  state_id_map[feature['properties']['name']] = feature['id']\n",
    "\n",
    "buyer_regions['id'] = buyer_regions['index'].apply(lambda x: state_id_map[x])\n",
    "\n",
    "#in order to get the right color scaling on the map\n",
    "buyer_regions['Yeezy orders scale'] = np.log10(buyer_regions['Yeezy'])\n",
    "buyer_regions['Total orders scale'] = np.log10(buyer_regions['Total orders'])\n",
    "\n",
    "#setting the map\n",
    "fig2 = px.choropleth(\n",
    "      buyer_regions,\n",
    "      locations = 'id',\n",
    "      geojson=states,\n",
    "      scope = 'usa',\n",
    "      color='Total orders scale',\n",
    "      hover_name = 'index',\n",
    "      hover_data =['Yeezy','Nike x OW','Total orders'],\n",
    "      color_continuous_scale = px.colors.sequential.GnBu,\n",
    "      title = 'Total Orders per State (Log Scale)'\n",
    "      )\n",
    "\n",
    "fig2.show()\n",
    "\n",
    "#positioning the pointing over each state the map is going to show: Name of the state, id numb, Yeezy Sales, Nike Sales, Total Sales, Total orders scale number\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 705
    },
    "id": "SBdf-UQXcyBK",
    "outputId": "158c77b5-9dc1-4a96-b366-32ff933681ea"
   },
   "outputs": [],
   "source": [
    "regions = df1[\"Buyer Region\"].unique()\n",
    "regions.sort()\n",
    "\n",
    "nike_top_10 = nike_models.loc[:9]\n",
    "\n",
    "nike_top_10_regions = nike_regions_count.head(10)\n",
    "nike_top_10_regions = pd.DataFrame({\"Region\": nike_top_10_regions.index, \"Count\": nike_top_10_regions.values})\n",
    "nike_top_10_regions.rename(columns={\"Region\": \"Buyer Region\"}, inplace=True)\n",
    "\n",
    "temp = nike[(nike[\"Buyer Region\"].isin(nike_top_10_regions[\"Buyer Region\"]) == True) & (nike[\"Sneaker Name\"].isin(nike_top_10[\"Sneaker Model\"]) == True)]\n",
    "temp = temp[[\"Sneaker Name\", \"Buyer Region\"]]\n",
    "\n",
    "temp = temp.groupby(by=[\"Sneaker Name\", \"Buyer Region\"]).size().to_frame().reset_index()\n",
    "temp.rename(columns={0: \"Count\"}, inplace=True)\n",
    "\n",
    "temp[\"Count_log\"] = np.log10(temp[\"Count\"])\n",
    "\n",
    "\n",
    "pivot = pd.pivot_table(temp, index=\"Sneaker Name\", columns=\"Buyer Region\", values=\"Count_log\")\n",
    "\n",
    "#chart\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "sns.heatmap(pivot,\n",
    "                 cmap=\"GnBu\",\n",
    "                 square=\"equal\",\n",
    "                 xticklabels=[\"CA\", \"FL\", \"IL\", \"MA\", \"NJ\", \"NY\", \"OR\", \"PA\", \"TX\", \"VA\"],\n",
    "                 yticklabels=[\"AJ1 UNC\", \"AF1 Volt\", \"Presto Black\", \"Presto White\", \"VaporMax 18\", \"VaporMax Black\", \"Blazer AHE\", \"Blazer GR\", \"Zoom Fly Black\", \"Zoom Fly Pink\"],\n",
    "                 \n",
    "                 )\n",
    "plt.title(\"Sneaker Model Sales in Different Region (Log Scale)\", fontdict={\"fontsize\":20}, pad=60)\n",
    "plt.xlabel(\"Buyer Region\", fontdict={\"fontsize\":15}, labelpad=20)\n",
    "plt.ylabel(\"Sneaker Name\", fontdict={\"fontsize\":15}, labelpad=20)\n",
    "print()\n",
    "plt.show()\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "______________________________________________________________________________________________________________________"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PHdjnaFnMdmj"
   },
   "source": [
    "**Notes**: some parts of step 2 and 3 have been reduced in order to meet the requirements. A complete work would require more intagrated visualization using more variables and models. \n",
    "\n",
    "  "
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "tfIspXuUHcg8",
    "594aXWUoHgrS"
   ],
   "name": "IT_Bella (local).ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
